# Code for our work "Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval", 

-- Necessary PACKAGES : PyTorch,  Tensorboard  and NLTK Punkt Sentence Tokenizer

# Evaluate Models
-- Download data and models from https://drive.google.com/drive/folders/1t3MwiCR72HDo6XiPvWSZpenqv4CGjnKl

-- To evaluate on MSR-VTT dataset : python test_weighted.py

# Reference 
If you use our code or models, please cite the following paper:

> @article{mithun2018learning, 
title={Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval}, author={Mithun, Niluthpol Chowdhury and Li, Juncheng and Metze, Florian and Roy-Chowdhury, Amit K}, year={2018}}

> Contact: Niluthpol C Mithun (nmith001@ucr.edu)

More Details and Code Coming Shortly
